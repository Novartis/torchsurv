{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing time-varying covariates\n",
    "\n",
    "In this notebook, we analyse a simulated dataset with time-varying covariates and survival outcomes. `TorchSurv` is used to train a model that predicts relative risk of subjects based on covariates observed over time. We will attempt to thoroughly explain the necessary elements to understand our implementation, but for a detailed read on time-varying survival models refer to Chapter 6 of [Dynamic Regression Models for Survival Data](https://link.springer.com/book/10.1007/0-387-33960-4). For a more brief explanation, please refer to these [slides](https://ms.uky.edu/~mai/sta635/Cox%20model.pdf). Below is a summary of the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future project ideas:\n",
    "Future projects can take on various themes: testing edge cases of this implementation, improving the code to become more robust to different data types, include weibull distribution and compare this approate.\n",
    "Testing edge cases:\n",
    "- use the simulated data and change different parameters to see how it affects performance, this can help guide appropriate use\n",
    "- design slightly different simulations known for being difficult or easy in specific scenarios\n",
    "- use a dataset with known properties\n",
    "\n",
    "Improving code to be more robust:\n",
    "- generalising the loss functions and overall defining the formatting required or it to work, generalise for different time scales etc.\n",
    "- extend the method to deal with multiple types of covariates in one loss function or a combination of multiple losses. This can extend to multiple time-varyin covariates and mixing time-invarint and varying ones.\n",
    "\n",
    "Weibull:\n",
    "- Extent the cox loss function to also include the Weibull distribution, this is described for both the log-likelihood and the simulation\n",
    "\n",
    "Comparison\n",
    "- One could compare this approach to other loss functions or statistical model to get an idea of what it brings as a benefit and a challenge. Note this comparison can be done via simulation or some dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial log likelihood for time-varying covariates\n",
    "\n",
    "### Context and statistical set-up\n",
    "\n",
    "Let $i$ e the index for some subject $i$ with a failute time denoted as $\\tau^*_i$ and $C$ be the censoring time. For the moment $C$ remains constant but there are extensions that allow for $C$ to vary over $i$. Let $\\tau_i = min(\\tau^*_i, C)$. We use $\\delta_i$ to denote whether $\\tau^*_i$ was observed. \n",
    "\n",
    "We will use $Z(t)$ to denote the value of of covariate $Z$ and time $t$. \n",
    "We use $Z(t)$ to denote the value of Z at time $t$ and $\\overline{Z}(t)$ to denote the set of covariates from the beginning up to time $t$: $ \\overline{Z}(t) = \\{ Z(s): 0 \\leq s \\leq t\\}$.\n",
    "Let $t_k$ for $k \\in \\{1, \\dots, K\\} denote the time points at which the covariates are observed. For the moment, we assume that all subjects have been observed on the same time grid. $R_k$ is the set of individuals who are at risk at $t_k$. \n",
    "\n",
    "The conditional hazard function of $T$ given $\\overline{Z}(t)$ is defined as\n",
    "$$ \\lambda(T|\\overline{Z}(t)) = Pr(T \\in [t, t+ dt)|T \\geq t, \\overline{Z}(t)), $$\n",
    "in other words, it is the probability that an event will occur in the next time instance if we have observed covariates up to time $t$ and that a subject has not yet experienced an event.\n",
    "\n",
    "The typical cox proportional hazards model with constant covariates $Z$ assumes a constant hazard ratio: $\\lambda(T|Z)= \\lambda_0(t) exp(\\beta Z)$, where $\\beta$ in an unknown set of regression parameters and $\\lambda_0(t)$ is an unspecified baseline hazard function. In this case $\\frac{\\lambda(T|Z)}{\\lambda_0(t)} = exp(\\beta Z) $. The cumulative hazard ia defined as $\\Lambda(t) = \\int_0^t \\lambda(s)ds$. \n",
    "\n",
    "In a time varying cox model, the hazard ratio is now dependent on time:\n",
    "$$ \\frac{\\lambda(t|Z)}{\\lambda_0(t)} = exp(\\beta Z(t)) $$ \n",
    "and the proportional hazard model specifies:\n",
    "$$ \\lambda(t|Z) = \\lambda_0(t)exp(\\beta Z(t)) $$\n",
    "\n",
    "Let $i_j$ denote the label or identity of the individual who fails at time $\\tau_j$, including the value of their time-varying covariate\n",
    "during their time in the study $\\{ Z_{i_j}(t): t \\in [0, \\tau_j] \\}$. The partial likelihood is:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\lambda(\\tau_j: Z_i(\\tau_j)))}{\\sum_{l \\in R_i} \\lambda(\\tau_j: Z_l(\\tau_j)))} \\Big),$$\n",
    "in terms of the model form:\n",
    "$$ L (\\beta) = \\prod_j \\Big (\\frac{\\exp(\\beta Z_i(\\tau_j))}{\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))} \\Big).$$\n",
    "\n",
    "Taking the log on both sides, we get the partial log-likelihood:\n",
    "$$ \\log L (\\beta) = \\sum_j \\Big (\\beta Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp(\\beta Z_i(\\tau_j))]\\Big ). $$\n",
    "\n",
    "\n",
    "### Extension to neural networks\n",
    "\n",
    "Consider a more genera form, where we have the cox proportional hazards model:\n",
    "$$\\lambda(T|\\overline{Z}(t))= \\lambda_0(t) \\theta(Z(t))$$\n",
    "\n",
    "Additionally, consider some network that maps the input covariates $Z(t)$ to the log relative hazards: $\\log \\theta(Z(t))$.\n",
    "\n",
    "The partial likelihood with respect to $\\theta(Z(\\tau_j))$ is written as:\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\log \\theta(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\theta (Z_i(\\tau_j))] \\Big).$$\n",
    "It onlu considers the covariate values at the time of event or censoring denoted as $\\tau_j$, all prior covariates are not considered.\n",
    "\n",
    "As the output of the network is set to be $\\log \\theta(Z(t))$, the code is written to account for this, to show this explicitly, set $\\phi(Z(t)) = \\log \\theta(Z(t))$ and write the log likelihood in terms oh $phi$:\n",
    "\n",
    "$$ \\log L(\\theta) = \\sum_j \\Big( \\phi(Z_i(\\tau_j)) - \\log [\\sum_{j \\in R_i} \\exp \\phi(Z_i(\\tau_j))] \\Big).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "To run this notebook, dependencies must be installed. the recommended method is to use our development conda environment (**preferred**). Instruction can be found [here](https://opensource.nibr.com/torchsurv/devnotes.html#set-up-a-development-environment-via-conda) to install all optional dependencies. The other method is to install only required packages using the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages (optional)\n",
    "# %pip install lifelines\n",
    "# %pip install matplotlib\n",
    "# %pip install sklearn\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Our package\n",
    "# from torchsurv.loss.time_varying import neg_partial_log_likelihood2\n",
    "# PyTorch boilerplate - see https://github.com/Novartis/torchsurv/blob/main/docs/notebooks/helpers_introduction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating realistic data\n",
    "\n",
    "A good approach for simulating data is described in detail by [Ngwa et al 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7731987/). If this is not yet implemented, it would be a good way of starting to ensure that both methods work as expected. There are tow parts in simulating such a dataset. First, simulating the longitudina lobservational data and then the survival data. Below we describe methodologies for both.\n",
    "\n",
    "### Longitudinal data (covariates)\n",
    "\n",
    "We use $i \\in \\{1, \\dots, n\\}$ to index subjects and $j \\in \\{1, \\dots, m_i\\}$ to index time points where $m_i$ is the final time point for subject $i$.\n",
    "We simulate covariates independently:\n",
    "- age at baseline $Age_i \\sim N(35,5)$\n",
    "- sex $\\sim Bernoulli(p=0.54)$\n",
    "\n",
    "Generate expected longitudinal trajectories $\\varphi_{\\beta}(t_{ij})$:\n",
    "\n",
    "$$ \\varphi_{\\beta}(t_{ij}) = b_{i1} + b_{i2} \\cdot t_{ij} + \\alpha Age_i, $$\n",
    "\n",
    "where $b_{i1}, b_{i2}$ are random effects\n",
    "\n",
    "We will generate $b_{i1}, b_{i2}$ from multivariate normal distribution with a covariance matrix $G = [[0.29, -0.00465],[-0.00465, 0.000320]]$. Sample from this multivariate normal distribution (with mean zero) to get the random intercept and slope.\n",
    "\n",
    "The observed longitudinal measures measures $Y_{ij}(t_{ij})$ from a multivariate normal distribution with mean $ \\varphi_{\\beta}(t_{ij})$ and variance $V$:\n",
    "\n",
    "$$ V = Z_i GZ_i ^T + R_i, \\text{ where }Z_i = [[1,1,1,1,1,1]^T, [0,5,10,15,20,25]^T]$$\n",
    "\n",
    "and $R_i = diag(\\sigma^2)$ and $\\sigma^2$ is set to $0.1161$.\n",
    "\n",
    "Note: Compared to the paper, we slightly adjust steps 3 and 4 from the simulation algorithm section (6.1) to avoid fitting a random effects model which adds more complexity in terms of data formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34.2016, 34.2186, 34.2356, 34.2526, 34.2696, 34.2866],\n",
      "        [33.4380, 33.4308, 33.4235, 33.4163, 33.4091, 33.4018],\n",
      "        [31.5581, 31.5564, 31.5548, 31.5531, 31.5515, 31.5498],\n",
      "        [35.7813, 35.7953, 35.8093, 35.8233, 35.8373, 35.8513]])\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n = 100  # Number of subjects\n",
    "T = torch.tensor(6)  # Number of time points\n",
    "time_vec = torch.tensor([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Simulation parameters\n",
    "age_mean = 35\n",
    "age_std = 5\n",
    "sex_prob = 0.54\n",
    "G = torch.tensor([[0.29, -0.00465], [-0.00465, 0.000320]])\n",
    "Z = torch.tensor([[1, 1, 1, 1, 1, 1], time_vec], dtype=torch.float32).T\n",
    "sigma = torch.tensor([0.1161])\n",
    "alpha = 1\n",
    "\n",
    "# Simulate age at baseline\n",
    "age_dist = dist.Normal(age_mean, age_std)\n",
    "age = age_dist.sample((n,))\n",
    "\n",
    "# Simulate sex\n",
    "sex_dist = dist.Bernoulli(probs=sex_prob)\n",
    "sex = sex_dist.sample((n,))\n",
    "\n",
    "# Simulate random effects\n",
    "random_effects_dist = dist.MultivariateNormal(torch.zeros(2), G)\n",
    "random_effects = random_effects_dist.sample((n,))\n",
    "\n",
    "# sample random error\n",
    "error_sample = dist.Normal(0, sigma).sample((n,))\n",
    "\n",
    "# Generate expected longitudinal trajectories\n",
    "# quite frakly this is useless now - it was based on my bad understanding of the algorithm\n",
    "trajectories = (\n",
    "    random_effects[:, 0].unsqueeze(1)\n",
    "    + random_effects[:, 1].unsqueeze(1) * Z[:, 1]\n",
    "    + alpha * age.unsqueeze(1)\n",
    "    + error_sample\n",
    ")\n",
    "\n",
    "print(trajectories[1:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANOTHER WAY OF GENERATING DATA\n",
    "\n",
    "# # Simulate observed longitudinal measures\n",
    "# R = torch.diag_embed(sigma.repeat(T))\n",
    "# V = torch.matmul(torch.matmul(Z, G), Z.T) + R\n",
    "\n",
    "# #get a mean trajectory\n",
    "# b1 = torch.tensor([4.250])\n",
    "# b2 = torch.tensor([0.250])\n",
    "# mean_trajectory =  b1.item() + b2.item() * Z[:,1] + alpha * age_mean\n",
    "\n",
    "# #define the distribution to sample the trajectories from\n",
    "# observed_data_dist = dist.MultivariateNormal(trajectories, V)\n",
    "\n",
    "# #sample from the distribution to get an n x T matrix of observations/covariates\n",
    "# observed_data = observed_data_dist.sample((1,)).squeeze()\n",
    "\n",
    "# print(observed_data[1:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival data (outcomes)\n",
    "\n",
    "here I will describe how to get the survival and censoring for all the subjects from above. then I will code it up in python.\n",
    "\n",
    "Specify (varying) values for the parameter estimates for $Age$, $sex$ and the link parameter $\\gamma$, which measures the strength of the association between the longitudinal measures $Y_{ij}(t_{ij})$ and the time-to-event $\\tau_j$.\n",
    "\n",
    "Let $Q \\sim Unif(0,1)$ be a random variable that determines the hazard of a subject. Then using the time varying cox model it can be expressd as:\n",
    "\n",
    "$$ Q(t;X,Y) = \\exp[-H_0(t)\\cdot \\exp(X^T\\alpha + \\gamma (b_{i1} + b_{i2} \\cdot t))],$$\n",
    "$X^T$ is a vector of tine-invariant covariates, $\\alpha$ a vector of regression coefficients.\n",
    "\n",
    "$H_o(t) = \\lambda t$ and if $h_0(t)>0$ for all $t$, then $H_0$ can be inverted:\n",
    "$$-\\log(Q) = \\lambda t \\cdot \\exp[X^T \\alpha + \\gamma (b_{i1} + b_{i2} \\cdot t) ] $$\n",
    "This expression can be rearranged to generate the times-to-event.\n",
    "\n",
    "Generate the time-to-event $\\tau_j$ using the following equations for the Cox Exponential model:\n",
    "$$ t = \\frac{1}{\\gamma \\cdot b_{i2}} W \\Big( \\frac{-\\gamma(b_{i2}) \\log(Q)}{\\lambda \\exp (X^T \\alpha + \\gamma(b_{i1}))} \\Big). $$\n",
    "\n",
    "Where $W$ is the Lambert W function (LWF) first proposed by [Corless et al. 1996](https://link.springer.com/article/10.1007/BF02124750) provide a history, theory and applications of the LWF. The LWF is the inverse of the function $f(p) = p \\cdot \\exp(p) $.\n",
    "\n",
    "Generate the censoring variable $C \\sim Unif⁡(25, 30)$ for censoring to occur later in study. From the survival and censoring times, we obtain the censoring indicator $\\delta_i$ which is defined as 1 if $\\tau_j < C_i$ and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lmbert W function\n",
    "\n",
    "from scipy.special import lambertw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: pre-determined parameters such as $\\alpha, \\gamma, \\lambda_0$ have a large effect on the event time outcomes, the values used here are:\n",
    "- $\\alpha_{age} = 0.05$,\n",
    "- $\\alpha_{sex} = -0.1$,\n",
    "- $\\gamma = 1.2$,\n",
    "- $\\lambda_0 = 0.04$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  3.,  8.,  5., 22.,  1., 12.,  7.,  7.,  2.,  3.,  3., 12.,  3.,\n",
      "        10.,  4., 17.,  1.,  3.,  2.,  1.,  3.,  1.,  1., 12.,  5.,  9.,  1.,\n",
      "         2.,  8.,  1.,  2.,  1.,  1.,  1.,  7.,  7., 13., 11.,  3.,  6., 10.,\n",
      "         5.,  1.,  3.,  3.,  3.,  4.,  1.,  1.,  3.,  2.,  2.,  2.,  5., 38.,\n",
      "         4.,  1.,  5.,  9.,  3.,  1., 19.,  4.,  5.,  4.,  4., 16., 15.,  1.,\n",
      "         1.,  2.,  4.,  9.,  2.,  3.,  6., 19.,  7.,  1.,  1.,  4.,  6.,  3.,\n",
      "         8.,  6.,  8.,  2.,  1.,  2., 32.,  1.,  2.,  1., 13.,  2.,  1.,  2.,\n",
      "         1.,  3.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "         True,  True, False,  True, False,  True,  True,  True,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "         True, False,  True, False, False, False,  True, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True, False,  True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the values for parameters, generate the random variables and call on relevant variables defined previously\n",
    "\n",
    "alpha = torch.tensor([0.05, -0.1])  # regression coefficient for time-invariant covariates\n",
    "gamma = torch.tensor(1.2)  # association strength between longitudinal measures and time-to-event\n",
    "lambda_0 = torch.tensor(0.04)  # baseline hazard rate\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Generate the random variables for hazard of a subject and censoring\n",
    "Q = dist.Uniform(0, 1).sample((n,))  # Random variable for hazard (Q)\n",
    "C = dist.Uniform(3, 5.5).sample((n,))  # Random variable for censoring\n",
    "\n",
    "# age and sex are the names of variables corresponding to those covariates\n",
    "# create the X matrix of covariates\n",
    "XX = torch.stack((age, sex), dim=1)\n",
    "\n",
    "# get b1 and b2 from the random sample we made before\n",
    "b1 = random_effects[:, 0]\n",
    "b2 = random_effects[:, 1]\n",
    "\n",
    "# Generate time to event T using the equation above\n",
    "log_Q = torch.log(Q)\n",
    "lambert_W_nominator = gamma * b2 * log_Q\n",
    "lambert_W_denominator = torch.exp(alpha @ XX.T + gamma * b1)\n",
    "# below should give a vector of length sample_size\n",
    "lambert_W = lambertw(-lambert_W_nominator / (lambda_0 * lambert_W_denominator))\n",
    "time_to_event = lambert_W / (gamma * b2)\n",
    "\n",
    "# take the real part of the LBF, the complex part is =0\n",
    "outcome_LWF = time_to_event.real\n",
    "outcome_LWF = torch.ceil(outcome_LWF)\n",
    "print(outcome_LWF)\n",
    "\n",
    "# implement censoring with some level of intensity below\n",
    "events = C < 5\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler method for generating the time-to-event where the covariate is assumed to have a more straightforward relation in time $Z(t) = kt$ for some $k>0$. This approach is suggested by [Peter C. Austin 2012](https://pmc.ncbi.nlm.nih.gov/articles/PMC3546387/pdf/sim0031-3946.pdf) and here \n",
    "$$ t = \\frac{1}{\\gamma k} \\log \\Big ( 1 + \\frac{\\gamma k (-log(u))}{\\lambda \\exp(\\alpha X)}\\Big). $$\n",
    "The above equation has been adapted to remain consistent with the parameters defined before. In our case, $k$ could be replaced with $b_{i2}$ if $b_{i2}$ would be sampled such that it is strictly positive. In the above configuration that is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Format\n",
    "\n",
    "Here we create a single matrix of data that corresponds to one covariate being observed over time for some dataset.\n",
    "The time series is padded with zeros so that each subject has the same length vector, the vector contains their covariate $Z_i(t)$ up until failure time $\\tau_j$ and then values beyond that are zero.\n",
    "\n",
    "In general, prior to fitting a survival model or a network, one should consider ohw to handle missing data beforehand. This is most important for covariates that are missing at event time $\\tau_j $. Data imputation methods can vary depending on the use case but some to consider are:\n",
    "- use the most recent value (assumes step function),\n",
    "- interpolate,\n",
    "- impute based on some model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN \n",
    "\n",
    "Below we will give an example set up of how to use the partial log likelihood in a loss function. We import the python file containing the loss and set up an RNN to work with our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import loss_time_covariates\n",
    "\n",
    "reload(loss_time_covariates)\n",
    "log_likelihood = loss_time_covariates._partial_likelihood_time_cox\n",
    "neg_loss_function = loss_time_covariates.neg_partial_time_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100, 1])\n",
      "torch.Size([6, 100, 1])\n",
      "torch.Size([2, 100, 1])\n",
      "torch.Size([6, 100, 1])\n",
      "loss = 1.0968555212020874, has gradient = True\n"
     ]
    }
   ],
   "source": [
    "# from torchsurv.loss import time_covariates\n",
    "# from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "\n",
    "# Parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "seq_length = T\n",
    "batch_size = n\n",
    "\n",
    "# Create simple RNN model\n",
    "rnn = torch.nn.RNN(input_size, output_size, num_layers)\n",
    "inputs = torch.randn(seq_length, batch_size, input_size)\n",
    "test = trajectories.T.unsqueeze(2)\n",
    "print(test.shape)\n",
    "print(inputs.shape)\n",
    "\n",
    "# initialize hidden state\n",
    "h0 = torch.randn(num_layers, batch_size, output_size)\n",
    "print(h0.shape)\n",
    "# Forward pass time series input\n",
    "outputs, _ = rnn(test, h0)\n",
    "print(outputs.shape)\n",
    "\n",
    "# outcome_LWF is the time someone experiences an event\n",
    "loss = neg_loss_function(outputs, outcome_LWF, events)\n",
    "print(f\"loss = {loss}, has gradient = {loss.requires_grad}\")  # loss = 1.0389232635498047, has gradient = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Lifelines package\n",
    "\n",
    "Re-format the simulation data to fit a normal time-varying cox model in the lifelines package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>events</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>37.006706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>37.013870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>37.021034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>37.028198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>34.201637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>34.218636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>34.235634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>33.438019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>33.430782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>33.423550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subj  start  stop  events        var\n",
       "0     0      0     1   False  37.006706\n",
       "1     0      1     2   False  37.013870\n",
       "2     0      2     3   False  37.021034\n",
       "3     0      3     4    True  37.028198\n",
       "4     1      0     1   False  34.201637\n",
       "5     1      1     2   False  34.218636\n",
       "6     1      2     3    True  34.235634\n",
       "7     2      0     1   False  33.438019\n",
       "8     2      1     2   False  33.430782\n",
       "9     2      2     3   False  33.423550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import sum as array_sum_to_scalar\n",
    "\n",
    "# as a reminder covars is the matrix of covariates where a row corresponds to a subject and a column corresponds to their observation at some time\n",
    "# the columns are padded so if a subject experiences an event, the remaining of the column is zero\n",
    "# Generating example torch matrix\n",
    "torch_matrix = trajectories\n",
    "# Convert torch matrix to pandas dataframe\n",
    "# set time to integer\n",
    "max_time = max(time_vec.type(torch.int64))\n",
    "print(max_time)\n",
    "event_time = outcome_LWF\n",
    "vars = []\n",
    "start = []\n",
    "stop = []\n",
    "event = []\n",
    "subjs = []\n",
    "\n",
    "for i in range(n):\n",
    "    subj_counter = 0\n",
    "    subj_event_time = int(event_time[i].item())\n",
    "    # print(subj_event_time)\n",
    "    for j in range(subj_event_time):\n",
    "        if j < max_time:\n",
    "            vars.append(torch_matrix[i, j].item())\n",
    "            start.append(j)\n",
    "            stop.append(j + 1)\n",
    "            event.append(False)\n",
    "            subjs.append(i)\n",
    "            subj_counter += 1\n",
    "        if j >= max_time:\n",
    "            vars.append(torch_matrix[i, -1].item())\n",
    "            start.append(j)\n",
    "            stop.append(subj_event_time)\n",
    "            event.append(False)\n",
    "            subjs.append(i)\n",
    "            subj_counter += 1\n",
    "            break\n",
    "    # set the last value to have an event\n",
    "    event[-1] = True\n",
    "    # if you want censoring use below\n",
    "    # if events[i]==True: event[-1]=True\n",
    "\n",
    "# for every time point before they experience an event\n",
    "# record all their variables until event time\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"subj\": subjs,\n",
    "        \"start\": start,\n",
    "        \"stop\": stop,\n",
    "        \"events\": event,\n",
    "        \"var\": vars,\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the lgo likelihood using the code from lifelines to compare our method to theirs. This snippet of code is taken from [cox_time_varying_fitter.py](https://github.com/CamDavidsonPilon/lifelines/blob/master/lifelines/fitters/cox_time_varying_fitter.py) on lines 499-550."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(76,)\n",
      "(62,)\n",
      "(47,)\n",
      "(38,)\n",
      "(32,)\n",
      "(28,)\n",
      "(23,)\n",
      "(19,)\n",
      "(16,)\n",
      "(14,)\n",
      "(13,)\n",
      "(10,)\n",
      "(8,)\n",
      "(7,)\n",
      "(6,)\n",
      "(5,)\n",
      "(3,)\n",
      "(2,)\n",
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_times = df[\"stop\"]\n",
    "event_bool = df[\"events\"]\n",
    "unique_death_times = np.unique(stop_times[event_bool])\n",
    "covariates = df[\"var\"]\n",
    "# the following is an internal column in lifelines, since we do not define it in this simulation it is set to 1.0\n",
    "# this is also done in the code at lines 182-185\n",
    "weights = np.ones(len(df))\n",
    "\n",
    "\n",
    "# below is defined at line 50, unsure what this means\n",
    "def matrix_axis_0_sum_to_1d_array(m):\n",
    "    return np.sum(m, 0)\n",
    "\n",
    "\n",
    "# we will be replacing x*beta from the code with out outputs from the network as written in the beginning of this notebook\n",
    "# network_out = outputs\n",
    "# print(network_out.shape)\n",
    "beta = np.array(\n",
    "    [1],\n",
    ")\n",
    "# print(beta)\n",
    "# print(beta.shape)\n",
    "# np.dot(X_at_t, beta)\n",
    "for t in unique_death_times:\n",
    "    # returns a boolean vector of length nxT in our case\n",
    "    ix = (start < t) & (t <= stop)\n",
    "    # returns a vector of covariates at event time\n",
    "    X_at_t = covariates[ix]\n",
    "    weights_at_t = weights[ix]\n",
    "    stops_events_at_t = stop_times[ix]\n",
    "    events_at_t = event_bool[ix]\n",
    "\n",
    "    # changed dot product to multiply cause dot is no longer supported in that way\n",
    "    phi_i = weights_at_t * np.exp(np.multiply(X_at_t, beta))\n",
    "    print(phi_i.shape)\n",
    "    # removed indexing from original code cause we only have 1 dim\n",
    "    phi_x_i = phi_i * X_at_t\n",
    "    phi_x_x_i = np.dot(X_at_t.T, phi_x_i)\n",
    "\n",
    "    # Calculate sums of Risk set\n",
    "    risk_phi = array_sum_to_scalar(phi_i)\n",
    "    risk_phi_x = matrix_axis_0_sum_to_1d_array(phi_x_i)\n",
    "    risk_phi_x_x = phi_x_x_i\n",
    "\n",
    "    # Calculate the sums of Tie set\n",
    "    deaths = events_at_t & (stops_events_at_t == t)\n",
    "\n",
    "    tied_death_counts = array_sum_to_scalar(deaths.astype(int))  # should always at least 1. Why? TODO\n",
    "\n",
    "    xi_deaths = X_at_t[deaths]\n",
    "\n",
    "    x_death_sum = matrix_axis_0_sum_to_1d_array(weights_at_t[deaths] * xi_deaths)\n",
    "\n",
    "    weight_count = array_sum_to_scalar(weights_at_t[deaths])\n",
    "    weighted_average = weight_count / tied_death_counts\n",
    "\n",
    "    # no tensors here, but do some casting to make it easier in the converging step next.\n",
    "    denom = 1.0 / np.array([risk_phi])\n",
    "    number = risk_phi_x\n",
    "    a1 = risk_phi_x_x * denom\n",
    "\n",
    "summand = number * denom[:, None]\n",
    "a2 = summand.T.dot(summand)\n",
    "log_lik = np.dot(x_death_sum, beta) + weighted_average * np.log(denom).sum()\n",
    "\n",
    "log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a cox regression model using the lifelines package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "ctv = CoxTimeVaryingFitter(penalizer=0)\n",
    "ctv.fit(df, id_col=\"subj\", event_col=\"events\", start_col=\"start\", stop_col=\"stop\", show_progress=True)\n",
    "ctv.print_summary()\n",
    "ctv.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real life data: heart transplant survival\n",
    "\n",
    "This is to demonstrate the method with a neural network, example inspired by the [lifelines example](https://lifelines.readthedocs.io/en/latest/Time%20varying%20survival%20regression.html#).\n",
    "\n",
    "This is a classic dataset for survival regression with time varying covariates. The original dataset is from J Crowley and M Hu. 'Covariance analysis of heart transplant survival data', and this dataset is from R’s survival library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "\n",
    "df = lifelines.datasets.load_stanford_heart_transplants()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following:\n",
    "\n",
    "- `start`: entry time,\n",
    "- `stop`: exit time,\n",
    "- `event`: status for this interval of time,\n",
    "- `age`: subjetct's age -48 years,\n",
    "- `year`: tyear of acceptance (in years after 1 Nov 1967)\n",
    "- `surgery`: prior bypass surgery 1=yes\n",
    "- `transplant`: received transplant 1=yes\n",
    "- `id`: patient id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsurv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
